"""
CodeAnalyzer åŠŸèƒ½æµ‹è¯•
æµ‹è¯•æ–‡ä»¶åˆ†ææ•´åˆã€æ ¼å¼åŒ–è¾“å‡ºç­‰åŠŸèƒ½
"""

import pytest
from unittest.mock import patch, MagicMock
from src.code_tokenizer.code_collector import CodeAnalyzer


class TestCodeAnalyzer:
    """CodeAnalyzer æµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        self.analyzer = CodeAnalyzer()

    def test_init(self):
        """æµ‹è¯• CodeAnalyzer åˆå§‹åŒ–"""
        assert hasattr(self.analyzer, 'file_analyzer')
        assert hasattr(self.analyzer, 'width_manager')
        assert self.analyzer.file_analyzer is not None
        assert self.analyzer.width_manager is not None

    def test_analyze_file(self, sample_python_file):
        """æµ‹è¯•æ–‡ä»¶åˆ†æåŠŸèƒ½"""
        result = self.analyzer.analyze_file(str(sample_python_file))

        # éªŒè¯è¿”å›çš„æ•°æ®ç»“æ„
        assert isinstance(result, dict)
        assert 'file_path' in result
        assert 'file_size' in result
        assert 'line_count' in result
        assert 'non_empty_line_count' in result
        assert 'char_count' in result
        assert 'word_count' in result
        assert 'token_count' in result
        assert 'token_count_gpt4' in result
        assert 'avg_tokens_per_line' in result
        assert 'small_lines_count' in result
        assert 'small_lines_percentage' in result
        assert 'context_analysis' in result

        # éªŒè¯åŸºæœ¬æ•°æ®çš„åˆç†æ€§
        assert result['file_path'] == str(sample_python_file)
        assert result['file_size'] > 0
        assert result['line_count'] >= 0
        assert result['char_count'] > 0
        assert result['word_count'] > 0
        assert result['token_count'] > 0
        assert result['token_count_gpt4'] > 0
        assert result['avg_tokens_per_line'] >= 0

    def test_analyze_file_javascript(self, sample_javascript_file):
        """æµ‹è¯• JavaScript æ–‡ä»¶åˆ†æ"""
        result = self.analyzer.analyze_file(str(sample_javascript_file))

        assert isinstance(result, dict)
        assert result['file_path'] == str(sample_javascript_file)
        assert result['file_size'] > 0
        assert result['char_count'] > 0
        assert result['word_count'] > 0
        assert result['token_count'] > 0

    def test_analyze_nonexistent_file(self):
        """æµ‹è¯•åˆ†æä¸å­˜åœ¨çš„æ–‡ä»¶"""
        with pytest.raises(FileNotFoundError):
            self.analyzer.analyze_file("/nonexistent/file.py")

    def test_format_bytes(self):
        """æµ‹è¯•å­—èŠ‚æ ¼å¼åŒ–"""
        # æµ‹è¯•ä¸åŒå¤§å°çš„å­—èŠ‚å€¼
        assert self.analyzer.format_bytes(0) == "0.00 B"
        assert self.analyzer.format_bytes(1023) == "1023.00 B"
        assert self.analyzer.format_bytes(1024) == "1.00 KB"
        assert self.analyzer.format_bytes(1536) == "1.50 KB"
        assert self.analyzer.format_bytes(1048576) == "1.00 MB"
        assert self.analyzer.format_bytes(1073741824) == "1.00 GB"

        # éªŒè¯è¿”å›ç±»å‹
        assert isinstance(self.analyzer.format_bytes(100), str)

    def test_format_bytes_negative(self):
        """æµ‹è¯•è´Ÿæ•°å­—èŠ‚æ ¼å¼åŒ–"""
        # è´Ÿæ•°åº”è¯¥è¢«æ­£ç¡®å¤„ç†
        result = self.analyzer.format_bytes(-100)
        assert isinstance(result, str)

    def test_print_analysis_basic(self, sample_python_file):
        """æµ‹è¯•åŸºæœ¬åˆ†æç»“æœæ‰“å°"""
        stats = self.analyzer.analyze_file(str(sample_python_file))

        # æ¨¡æ‹Ÿ console æ‰“å°
        with patch('src.code_tokenizer.code_collector.console') as mock_console:
            self.analyzer.print_analysis(str(sample_python_file), stats)

            # éªŒè¯ console.print è¢«è°ƒç”¨
            assert mock_console.print.called
            assert mock_console.print.call_count >= 1

    def test_print_analysis_with_context_data(self, sample_python_file):
        """æµ‹è¯•åŒ…å«ä¸Šä¸‹æ–‡æ•°æ®çš„åˆ†æç»“æœæ‰“å°"""
        stats = self.analyzer.analyze_file(str(sample_python_file))

        # ç¡®ä¿ context_analysis æœ‰æ•°æ®
        assert 'context_analysis' in stats
        assert len(stats['context_analysis']) > 0

        with patch('src.code_tokenizer.code_collector.console') as mock_console:
            self.analyzer.print_analysis(str(sample_python_file), stats)

            # éªŒè¯è°ƒç”¨äº†æ­£ç¡®çš„æ‰“å°æ–¹æ³•
            assert mock_console.print.called

    def test_print_analysis_large_file(self, temp_dir):
        """æµ‹è¯•å¤§æ–‡ä»¶çš„åˆ†æç»“æœæ‰“å°"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ›´å¤šå†…å®¹çš„æ–‡ä»¶
        large_content = '''
# è¿™æ˜¯ä¸€ä¸ªå¤§å‹æµ‹è¯•æ–‡ä»¶
import os
import sys
import json
from typing import Dict, List, Optional

def process_data(data: List[Dict]) -> Dict:
    """å¤„ç†æ•°æ®"""
    result = {}
    for item in data:
        key = item.get('key')
        value = item.get('value')
        if key and value:
            result[key] = value
    return result

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""

    def __init__(self, config: Dict):
        self.config = config
        self.processed_items = []

    def process_item(self, item: Dict) -> bool:
        """å¤„ç†å•ä¸ªé¡¹ç›®"""
        try:
            processed = process_data([item])
            self.processed_items.append(processed)
            return True
        except Exception as e:
            print(f"Error processing item: {e}")
            return False

    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_items': len(self.processed_items),
            'config': self.config
        }

def main():
    """ä¸»å‡½æ•°"""
    config = {'debug': True, 'version': '1.0'}
    processor = DataProcessor(config)

    test_data = [
        {'key': 'test1', 'value': 100},
        {'key': 'test2', 'value': 200},
        {'key': 'test3', 'value': 300}
    ]

    for item in test_data:
        processor.process_item(item)

    stats = processor.get_statistics()
    print(f"Processed {stats['total_items']} items")

if __name__ == "__main__":
    main()
'''
        large_file = temp_dir / "large_test.py"
        large_file.write_text(large_content)

        stats = self.analyzer.analyze_file(str(large_file))

        with patch('src.code_tokenizer.code_collector.console') as mock_console:
            self.analyzer.print_analysis(str(large_file), stats)

            # éªŒè¯èƒ½å¤Ÿå¤„ç†å¤§æ–‡ä»¶çš„åˆ†æç»“æœ
            assert mock_console.print.called

    def test_print_analysis_with_special_characters(self, temp_dir):
        """æµ‹è¯•åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡ä»¶åˆ†ææ‰“å°"""
        special_content = '''
# æµ‹è¯•ç‰¹æ®Šå­—ç¬¦æ–‡ä»¶
def test_special_chars():
    """æµ‹è¯•å„ç§ç‰¹æ®Šå­—ç¬¦"""
    special_string = "Hello ä¸–ç•Œ! @#$%^&*()_+-=[]{}|;':\",./<>?"
    unicode_chars = "Emoji: ğŸš€ ğŸ‰ â­"
    chinese_text = "è¿™æ˜¯ä¸­æ–‡æµ‹è¯•å†…å®¹"

    return {
        'special': special_string,
        'unicode': unicode_chars,
        'chinese': chinese_text
    }

# æµ‹è¯•æ•°å­¦ç¬¦å·
math_symbols = "âˆ‘âˆâˆ«âˆ†âˆ‡âˆ‚âˆÂ±Ã—Ã·â‰ â‰¤â‰¥â‰ˆâˆ"
# æµ‹è¯•å¼•å·
quotes = "'å•å¼•å·' \"åŒå¼•å·\" `åå¼•å·`"
'''
        special_file = temp_dir / "special_chars.py"
        special_file.write_text(special_content)

        stats = self.analyzer.analyze_file(str(special_file))

        with patch('src.code_tokenizer.code_collector.console') as mock_console:
            self.analyzer.print_analysis(str(special_file), stats)

            # éªŒè¯èƒ½å¤Ÿå¤„ç†ç‰¹æ®Šå­—ç¬¦
            assert mock_console.print.called

    def test_print_analysis_error_handling(self, sample_python_file):
        """æµ‹è¯•åˆ†ææ‰“å°ä¸­çš„é”™è¯¯å¤„ç†"""
        stats = self.analyzer.analyze_file(str(sample_python_file))

        # æµ‹è¯•ä¸ä¼šå› ä¸ºæ•°æ®é—®é¢˜è€Œå´©æºƒ
        # ç”±äº print_analysis ç›´æ¥ä½¿ç”¨ consoleï¼Œæˆ‘ä»¬ä¸»è¦æµ‹è¯•åˆ†ææ•°æ®çš„æœ‰æ•ˆæ€§
        assert isinstance(stats, dict)
        assert 'file_path' in stats

        # æµ‹è¯•åˆ†æåŠŸèƒ½æœ¬èº«ä¸ä¼šå‡ºé”™
        result = self.analyzer.analyze_file(str(sample_python_file))
        assert result == stats

    def test_print_analysis_with_empty_stats(self):
        """æµ‹è¯•ä½¿ç”¨ç©ºç»Ÿè®¡æ•°æ®çš„åˆ†ææ‰“å°"""
        empty_stats = {
            'file_path': '/test/empty.py',
            'file_size': 0,
            'line_count': 0,
            'non_empty_line_count': 0,
            'char_count': 0,
            'word_count': 0,
            'token_count': 0,
            'token_count_gpt4': 0,
            'avg_tokens_per_line': 0,
            'small_lines_count': 0,
            'small_lines_percentage': 0,
            'context_analysis': {}
        }

        with patch('src.code_tokenizer.code_collector.console') as mock_console:
            self.analyzer.print_analysis('/test/empty.py', empty_stats)

            # å³ä½¿æ•°æ®ä¸ºç©ºä¹Ÿåº”è¯¥èƒ½å¤Ÿæ­£å¸¸æ‰“å°
            assert mock_console.print.called

    def test_print_analysis_with_large_token_count(self, temp_dir):
        """æµ‹è¯•å¤§ token æ•°é‡çš„åˆ†ææ‰“å°"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«å¤§é‡å†…å®¹çš„æ–‡ä»¶
        large_content = "# Large content file\n" + "print('line')\n" * 1000
        large_file = temp_dir / "large_tokens.py"
        large_file.write_text(large_content)

        stats = self.analyzer.analyze_file(str(large_file))

        # éªŒè¯ token æ•°é‡è¾ƒå¤§
        assert stats['token_count'] > 1000

        with patch('src.code_tokenizer.code_collector.console') as mock_console:
            self.analyzer.print_analysis(str(large_file), stats)

            # åº”è¯¥èƒ½å¤Ÿå¤„ç†å¤§ token æ•°é‡
            assert mock_console.print.called

    def test_print_analysis_context_window_exceeded(self, temp_dir):
        """æµ‹è¯•ä¸Šä¸‹æ–‡çª—å£è¶…é™çš„åˆ†ææ‰“å°"""
        # åˆ›å»ºä¸€ä¸ªå†…å®¹å¾ˆé•¿çš„æ–‡ä»¶ï¼Œå¯èƒ½å¯¼è‡´æŸäº›æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£è¶…é™
        very_long_content = "# Very long content file\n"
        very_long_content += "x" * 100000  # å¤§é‡å­—ç¬¦

        very_long_file = temp_dir / "very_long.py"
        very_long_file.write_text(very_long_content)

        stats = self.analyzer.analyze_file(str(very_long_file))

        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£è¶…é™
        context_analysis = stats.get('context_analysis', {})
        has_exceeded = any(info.get('exceeded', False) for info in context_analysis.values())

        with patch('src.code_tokenizer.code_collector.console') as mock_console:
            self.analyzer.print_analysis(str(very_long_file), stats)

            # åº”è¯¥èƒ½å¤Ÿå¤„ç†ä¸Šä¸‹æ–‡çª—å£è¶…é™çš„æƒ…å†µ
            assert mock_console.print.called

    def test_integration_with_file_analyzer(self, sample_python_file):
        """æµ‹è¯•ä¸ FileAnalyzer çš„é›†æˆ"""
        # ç›´æ¥è°ƒç”¨ CodeAnalyzer çš„ analyze_file æ–¹æ³•
        result1 = self.analyzer.analyze_file(str(sample_python_file))

        # ç›´æ¥è°ƒç”¨ FileAnalyzer çš„ analyze_file æ–¹æ³•
        result2 = self.analyzer.file_analyzer.analyze_file(str(sample_python_file))

        # ç»“æœåº”è¯¥ç›¸åŒ
        assert result1 == result2

    def test_integration_with_context_window_summary(self, sample_python_file):
        """æµ‹è¯•ä¸ä¸Šä¸‹æ–‡çª—å£æ‘˜è¦çš„é›†æˆ"""
        stats = self.analyzer.analyze_file(str(sample_python_file))
        token_count = stats['token_count']

        # ç›´æ¥è·å–ä¸Šä¸‹æ–‡çª—å£æ‘˜è¦
        summary = self.analyzer.file_analyzer.get_context_window_summary(token_count)

        # éªŒè¯æ‘˜è¦æ•°æ®
        assert isinstance(summary, list)
        assert len(summary) > 0

        for item in summary:
            assert 'model' in item
            assert 'percentage' in item
            assert 'token_count' in item
            assert 'limit' in item
            assert 'exceeded' in item

    def test_multiple_file_analysis(self, sample_python_file, sample_javascript_file):
        """æµ‹è¯•å¤šæ–‡ä»¶åˆ†æ"""
        # åˆ†æå¤šä¸ªæ–‡ä»¶
        py_stats = self.analyzer.analyze_file(str(sample_python_file))
        js_stats = self.analyzer.analyze_file(str(sample_javascript_file))

        # éªŒè¯ä¸¤ä¸ªæ–‡ä»¶çš„åˆ†æç»“æœ
        assert py_stats['file_path'] != js_stats['file_path']
        assert py_stats['file_size'] > 0
        assert js_stats['file_size'] > 0

        # éªŒè¯å¯ä»¥åˆ†åˆ«æ‰“å°
        with patch('src.code_tokenizer.code_collector.console') as mock_console:
            self.analyzer.print_analysis(str(sample_python_file), py_stats)
            self.analyzer.print_analysis(str(sample_javascript_file), js_stats)

            # åº”è¯¥è°ƒç”¨æ‰“å°æ–¹æ³• 4 æ¬¡ï¼ˆæ¯ä¸ªæ–‡ä»¶ 2 æ¬¡ï¼šPanel + Tableï¼‰
            assert mock_console.print.call_count == 4

    def test_analysis_consistency(self, sample_python_file):
        """æµ‹è¯•åˆ†æç»“æœçš„ä¸€è‡´æ€§"""
        # å¤šæ¬¡åˆ†æåŒä¸€ä¸ªæ–‡ä»¶åº”è¯¥å¾—åˆ°ç›¸åŒç»“æœ
        result1 = self.analyzer.analyze_file(str(sample_python_file))
        result2 = self.analyzer.analyze_file(str(sample_python_file))

        # é™¤äº†å¯èƒ½çš„æ—¶é—´æˆ³ç›¸å…³å­—æ®µï¼Œå…¶ä»–å­—æ®µåº”è¯¥ç›¸åŒ
        consistent_fields = [
            'file_path', 'file_size', 'line_count', 'non_empty_line_count',
            'char_count', 'word_count', 'token_count', 'token_count_gpt4',
            'avg_tokens_per_line', 'small_lines_count', 'small_lines_percentage'
        ]

        for field in consistent_fields:
            assert result1[field] == result2[field], f"Field {field} should be consistent"

    def test_analysis_with_different_file_types(self, temp_dir):
        """æµ‹è¯•ä¸åŒæ–‡ä»¶ç±»å‹çš„åˆ†æ"""
        # åˆ›å»ºä¸åŒç±»å‹çš„æµ‹è¯•æ–‡ä»¶
        files_content = {
            'test.py': 'print("Hello Python")\ndef func():\n    return 42',
            'test.js': 'console.log("Hello JavaScript");\nfunction func() {\n    return 42;\n}',
            'test.md': '# Hello Markdown\n\nThis is a test file.\n\n## Section 2',
            'test.txt': 'Hello Text File\nThis is plain text.',
            'test.json': '{"hello": "world", "number": 42}'
        }

        results = {}
        for filename, content in files_content.items():
            file_path = temp_dir / filename
            file_path.write_text(content)
            results[filename] = self.analyzer.analyze_file(str(file_path))

        # éªŒè¯æ‰€æœ‰æ–‡ä»¶éƒ½èƒ½è¢«åˆ†æ
        for filename, result in results.items():
            assert isinstance(result, dict)
            assert result['file_path'].endswith(filename)
            assert result['file_size'] > 0
            assert result['char_count'] > 0

        # éªŒè¯ä¸åŒæ–‡ä»¶ç±»å‹æœ‰ä¸åŒçš„ç‰¹å¾
        py_result = results['test.py']
        js_result = results['test.js']
        md_result = results['test.md']

        # Python å’Œ JavaScript æ–‡ä»¶åº”è¯¥æœ‰ç±»ä¼¼çš„ token æ•°é‡ï¼ˆå†…å®¹ç›¸ä¼¼ï¼‰
        assert abs(py_result['token_count'] - js_result['token_count']) < 50

        # Markdown æ–‡ä»¶å¯èƒ½æœ‰ä¸åŒçš„ token æ¯”ä¾‹
        assert md_result['token_count'] > 0

    def test_print_analysis_table_format(self, sample_python_file):
        """æµ‹è¯•åˆ†æç»“æœè¡¨æ ¼æ ¼å¼"""
        stats = self.analyzer.analyze_file(str(sample_python_file))

        # ç®€åŒ–æµ‹è¯•ï¼šä¸»è¦éªŒè¯åˆ†æç»“æœåŒ…å«æ‰€éœ€æ•°æ®
        assert 'context_analysis' in stats
        assert isinstance(stats['context_analysis'], dict)
        assert len(stats['context_analysis']) > 0

        # éªŒè¯ä¸Šä¸‹æ–‡çª—å£æ•°æ®ç»“æ„
        for model_name, info in stats['context_analysis'].items():
            assert 'limit' in info
            assert 'token_count' in info
            assert 'percentage' in info
            assert 'exceeded' in info